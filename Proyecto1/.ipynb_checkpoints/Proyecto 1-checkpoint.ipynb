{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juandiegocardona/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Gr√°ficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#style.use('ggplot') or plt.style.use('ggplot')\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Configuraci√≥n warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lectura de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mensaje</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling rather rotten so im not very ambiti...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im updating my blog because i feel shitty</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i never make her separate from me because i do...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was feeling a little vain when i did this one</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>i just keep feeling like someone is being unki...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>im feeling a little cranky negative after this...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>i feel that i am useful to my people and that ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>im feeling more comfortable with derby i feel ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>i feel all weird when i have to meet w people ...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                mensaje sentimiento\n",
       "0     im feeling rather rotten so im not very ambiti...     sadness\n",
       "1             im updating my blog because i feel shitty     sadness\n",
       "2     i never make her separate from me because i do...     sadness\n",
       "3     i left with my bouquet of red and yellow tulip...         joy\n",
       "4       i was feeling a little vain when i did this one     sadness\n",
       "...                                                 ...         ...\n",
       "1995  i just keep feeling like someone is being unki...       anger\n",
       "1996  im feeling a little cranky negative after this...       anger\n",
       "1997  i feel that i am useful to my people and that ...         joy\n",
       "1998  im feeling more comfortable with derby i feel ...         joy\n",
       "1999  i feel all weird when i have to meet w people ...        fear\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datos.txt',sep=';',header=None)\n",
    "df.columns = ['mensaje','sentimiento']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza y Tokenizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso de limpieza de texto, dentro del √°mbito de text mining, consiste en eliminar del texto todo aquello que no aporte informaci√≥n sobre su tem√°tica, estructura o contenido. No existe una √∫nica forma de hacerlo, depende en gran medida de la finalidad del an√°lisis y de la fuente de la que proceda el texto. Por ejemplo, en las redes sociales, los usuarios pueden escribir de la forma que quieran, lo que suele resultar en un uso elevado de abreviaturas y signos de puntuaci√≥n. En este ejercicio, se procede a eliminar: patrones no informativos (urls de p√°ginas web), signos de puntuaci√≥n, etiquetas HTML, caracteres sueltos y n√∫meros.\n",
    "\n",
    "Tokenizar un texto consiste en dividir el texto en las unidades que lo conforman, entendiendo por unidad el elemento m√°s sencillo con significado propio para el an√°lisis en cuesti√≥n, en este caso, las palabras.\n",
    "\n",
    "Existen m√∫ltiples librer√≠as que automatizan en gran medida la limpieza y tokenizaci√≥n de texto, por ejemplo, la clase feature_extraction.text.CountVectorizer de Scikit Learn, nltk.tokenize o spaCy. A pesar de ello, para este ejemplo, se define una funci√≥n que, si bien est√° menos optimizada, tiene la ventaja de poder adaptarse f√°cilmente dependiendo del tipo de texto analizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto es 1 ejemplo de l'limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\n",
      "['esto', 'es', 'ejemplo', 'de', 'limpieza', 'de', 'texto', 'cienciadedatos', 'textmining']\n"
     ]
    }
   ],
   "source": [
    "def limpiar_tokenizar(mensaje):\n",
    "    '''\n",
    "    Esta funci√≥n limpia y tokeniza el texto en palabras individuales.\n",
    "    El orden en el que se va limpiando el texto no es arbitrario.\n",
    "    El listado de signos de puntuaci√≥n se ha obtenido de: print(string.punctuation)\n",
    "    y re.escape(string.punctuation)\n",
    "    '''\n",
    "    \n",
    "    # Se convierte todo el texto a min√∫sculas\n",
    "    nuevo_texto = mensaje.lower()\n",
    "    # Eliminaci√≥n de p√°ginas web (palabras que empiezan por \"http\")\n",
    "    nuevo_texto = re.sub('http\\S+', ' ', nuevo_texto)\n",
    "    # Eliminaci√≥n de signos de puntuaci√≥n\n",
    "    regex = '[\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
    "    nuevo_texto = re.sub(regex , ' ', nuevo_texto)\n",
    "    # Eliminaci√≥n de n√∫meros\n",
    "    nuevo_texto = re.sub(\"\\d+\", ' ', nuevo_texto)\n",
    "    # Eliminaci√≥n de espacios en blanco m√∫ltiples\n",
    "    nuevo_texto = re.sub(\"\\\\s+\", ' ', nuevo_texto)\n",
    "    # Tokenizaci√≥n por palabras individuales\n",
    "    nuevo_texto = nuevo_texto.split(sep = ' ')\n",
    "    # Eliminaci√≥n de tokens con una longitud < 2\n",
    "    nuevo_texto = [token for token in nuevo_texto if len(token) > 1]\n",
    "    \n",
    "    return(nuevo_texto)\n",
    "\n",
    "test = \"Esto es 1 ejemplo de l'limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\"\n",
    "print(test)\n",
    "print(limpiar_tokenizar(mensaje=test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mensaje</th>\n",
       "      <th>mensaje_tokenizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling rather rotten so im not very ambiti...</td>\n",
       "      <td>[im, feeling, rather, rotten, so, im, not, ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im updating my blog because i feel shitty</td>\n",
       "      <td>[im, updating, my, blog, because, feel, shitty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i never make her separate from me because i do...</td>\n",
       "      <td>[never, make, her, separate, from, me, because...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>[left, with, my, bouquet, of, red, and, yellow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was feeling a little vain when i did this one</td>\n",
       "      <td>[was, feeling, little, vain, when, did, this, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             mensaje  \\\n",
       "0  im feeling rather rotten so im not very ambiti...   \n",
       "1          im updating my blog because i feel shitty   \n",
       "2  i never make her separate from me because i do...   \n",
       "3  i left with my bouquet of red and yellow tulip...   \n",
       "4    i was feeling a little vain when i did this one   \n",
       "\n",
       "                                  mensaje_tokenizado  \n",
       "0  [im, feeling, rather, rotten, so, im, not, ver...  \n",
       "1    [im, updating, my, blog, because, feel, shitty]  \n",
       "2  [never, make, her, separate, from, me, because...  \n",
       "3  [left, with, my, bouquet, of, red, and, yellow...  \n",
       "4  [was, feeling, little, vain, when, did, this, ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se aplica la funci√≥n de limpieza y tokenizaci√≥n a cada mensaje\n",
    "# ==============================================================================\n",
    "df['mensaje_tokenizado'] = df['mensaje'].apply(lambda x: limpiar_tokenizar(x))\n",
    "df[['mensaje', 'mensaje_tokenizado']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## An√°lisis exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de entender que caracteriza la escritura de cada autor, es interesante estudiar qu√© palabras emplea, con qu√© frecuencia, as√≠ como el significado de las mismas.\n",
    "\n",
    "En Python, una de las estructuras que m√°s facilita el an√°lisis exploratorio es el DataFrame de Pandas, que es la estructura en la que se encuentra almacenada ahora la informaci√≥n de los tweets. Sin embargo, al realizar la tokenizaci√≥n, ha habido un cambio importante. Antes de dividir el texto, los elementos de estudio eran los tweets, y cada uno se encontraba en una fila, cumplimento as√≠ la condici√≥n de tidy data: una observaci√≥n, una fila. Al realizar la tokenizaci√≥n, el elemento de estudio ha pasado a ser cada token (palabra), incumpliendo as√≠ la condici√≥n de tidy data. Para volver de nuevo a la estructura ideal se tiene que expandir cada lista de tokens, duplicando el valor de las otras columnas tantas veces como sea necesario. A este proceso se le conoce como expansi√≥n o unnest.\n",
    "\n",
    "Aunque puede parecer un proceso poco eficiente (el n√∫mero de filas aumenta mucho), este simple cambio facilita actividades de tipo: agrupaci√≥n, contaje, gr√°ficos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimiento</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>im</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>feeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>rather</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentimiento    token\n",
       "0     sadness       im\n",
       "0     sadness  feeling\n",
       "0     sadness   rather"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unnest de la columna texto_tokenizado\n",
    "# ==============================================================================\n",
    "df_tidy = df.explode(column='mensaje_tokenizado')\n",
    "df_tidy = df_tidy.drop(columns='mensaje')\n",
    "df_tidy = df_tidy.rename(columns={'mensaje_tokenizado':'token'})\n",
    "df_tidy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras totales utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34073"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tidy['token'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras totales distintas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4778"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tidy['token'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras m√°s utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    " # df_tidy.groupby(['token'])[\"token\"] \\\n",
    " # .count() \\\n",
    " # .reset_index(name='count') \\\n",
    " # .apply(lambda x: x.sort_values('count', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la tabla anterior puede observarse que los t√©rminos m√°s frecuentes en todos los usuarios se corresponden con art√≠culos, preposiciones, pronombres‚Ä¶, en general, palabras que no aportan informaci√≥n relevante sobre el texto. Ha estas palabras se les conoce como stopwords. Para cada idioma existen distintos listados de stopwords, adem√°s, dependiendo del contexto, puede ser necesario adaptar el listado. Por ejemplo, en la tabla anterior aparece el t√©rmino amp que procede de la etiqueta html &amp. Con frecuencia, a medida que se realiza un an√°lisis se encuentran palabras que deben incluirse en el listado de stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# Obtenci√≥n de listado de stopwords del ingl√©s\n",
    "# ==============================================================================\n",
    "stop_words = list(stopwords.words('english'))\n",
    "# Se a√±ade la stoprword: amp, ax, ex\n",
    "stop_words.extend((\"amp\", \"xa\", \"xe\"))\n",
    "print(stop_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrado para excluir stopwords\n",
    "# ==============================================================================\n",
    "df_tidy = df_tidy[~(df_tidy[\"token\"].isin(stop_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Top 10 palabras (sin stopwords)\n",
    "# ==============================================================================\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1,figsize=(6, 7))\n",
    "counts  = df_tidy['token'].value_counts(ascending=False).head(10)\n",
    "counts.plot(kind='barh', color='firebrick')\n",
    "axs.invert_yaxis()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency e Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los principales intereses en text mining, natural language processing e information retrieval es cuantificar la tem√°tica de un texto, as√≠ como la importancia de cada t√©rmino que lo forma. Una manera sencilla de medir la importancia de un t√©rmino dentro de un documento es utilizando la frecuencia con la que aparece (tf, term-frequency). Esta aproximaci√≥n, aunque simple, tiene la limitaci√≥n de atribuir mucha importancia a aquellas palabras que aparecen muchas veces aunque no aporten informaci√≥n selectiva. Por ejemplo, si la palabra matem√°ticas aparece 5 veces en un documento y la palabra p√°gina aparece 50, la segunda tendr√° 10 veces m√°s peso a pesar de que no aporte tanta informaci√≥n sobre la tem√°tica del documento. Para solucionar este problema se pueden ponderar los valores tf multiplic√°ndolos por la inversa de la frecuencia con la que el t√©rmino en cuesti√≥n aparece en el resto de documentos(idf). De esta forma, se consigue reducir el valor de aquellos t√©rminos que aparecen en muchos documentos y que, por lo tanto, no aportan informaci√≥n selectiva.\n",
    "\n",
    "El estad√≠stico tf-idf mide c√≥mo de informativo es un t√©rmino en un documento teniendo en cuenta la frecuencia con la que ese t√©rmino aparece en otros documentos.\n",
    "\n",
    "\n",
    "\n",
    "Term Frequency (tf)\n",
    "\n",
    "tf (t, d)=ùëõtlongitud d\n",
    " \n",
    "donde  ùëõt  es el n√∫mero de veces que aparece el t√©rmino  ùë°  en el documento  ùëë .\n",
    "\n",
    "Inverse Document Frequency\n",
    "\n",
    "idf (t)=log(ùëõdùëõ(d,t))\n",
    " \n",
    "donde  ùëõd  es el n√∫mero total de documentos y  ùëõ(d,t)  el n√∫mero de documentos que contienen el t√©rmino  ùë° .\n",
    "\n",
    "Estad√≠stico tf-idf\n",
    "\n",
    "tf-idf(t, d)=tf (t, d)‚àóidf (t)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "En la pr√°ctica, para evitar problemas con el logaritmo cuando aparecen valores de 0, se emplea una versi√≥n corregida del  idf (t) . Esta es la versi√≥n implementada en Scikit Learn.\n",
    "\n",
    "idf (t)=log1+ùëõùëë1+ùëõ(d,t)+1\n",
    " \n",
    "\n",
    "\n",
    "En los siguientes apartados se muestra c√≥mo calcular el valor tf-idf Sin embargo, en la pr√°ctica, es preferible utilizar implementaciones como TfidfVectorizer de Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaah</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>powers</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>powerless</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token  count  total_count   tf\n",
       "0         aaaah      1            1  1.0\n",
       "3099     powers      1            1  1.0\n",
       "3098  powerless      1            1  1.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C√°lculo term-frecuency (tf)\n",
    "# ==============================================================================\n",
    "tf = df_tidy.copy()\n",
    "# N√∫mero de veces que aparece cada t√©rmino en cada tweet\n",
    "tf = tf.groupby([\"token\"])[\"token\"].agg([\"count\"]).reset_index()\n",
    "# Se a√±ade una columna con el total de t√©rminos por tweet\n",
    "tf['total_count'] = tf.groupby('token')['count'].transform(sum)\n",
    "# Se calcula el tf\n",
    "tf['tf'] = tf[\"count\"] / tf[\"total_count\"]\n",
    "tf.sort_values(by = \"tf\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>n_documentos</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>feel</td>\n",
       "      <td>1394</td>\n",
       "      <td>1.203399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>feeling</td>\n",
       "      <td>646</td>\n",
       "      <td>1.972532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>like</td>\n",
       "      <td>373</td>\n",
       "      <td>2.521753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        token  n_documentos       idf\n",
       "1515     feel          1394  1.203399\n",
       "1516  feeling           646  1.972532\n",
       "2371     like           373  2.521753"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inverse document frequency\n",
    "# ==============================================================================\n",
    "idf = df_tidy.copy()\n",
    "total_documents = idf[\"token\"].drop_duplicates().count()\n",
    "# N√∫mero de documentos (tweets) en los que aparece cada t√©rmino\n",
    "idf = idf.groupby([\"token\"])[\"token\"].agg([\"count\"]).reset_index()\n",
    "idf['n_documentos'] = idf.groupby('token')['count'].transform(sum)\n",
    "# C√°lculo del idf\n",
    "idf['idf'] = np.log(total_documents / idf['n_documentos'])\n",
    "idf = idf[[\"token\",\"n_documentos\", \"idf\"]].drop_duplicates()\n",
    "idf.sort_values(by=\"idf\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>tf</th>\n",
       "      <th>n_documentos</th>\n",
       "      <th>idf</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaah</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.443331</td>\n",
       "      <td>8.443331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7.344719</td>\n",
       "      <td>7.344719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandoning</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.443331</td>\n",
       "      <td>8.443331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.443331</td>\n",
       "      <td>8.443331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abba</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.443331</td>\n",
       "      <td>8.443331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token  count  total_count   tf  n_documentos       idf    tf_idf\n",
       "0        aaaah      1            1  1.0             1  8.443331  8.443331\n",
       "1    abandoned      3            3  1.0             3  7.344719  7.344719\n",
       "2   abandoning      1            1  1.0             1  8.443331  8.443331\n",
       "3  abandonment      1            1  1.0             1  8.443331  8.443331\n",
       "4         abba      1            1  1.0             1  8.443331  8.443331"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency - Inverse Document Frequency\n",
    "# ==============================================================================\n",
    "tf_idf = pd.merge(left=tf, right=idf, on=\"token\")\n",
    "tf_idf[\"tf_idf\"] = tf_idf[\"tf\"] * tf_idf[\"idf\"]\n",
    "tf_idf.sort_values(by=\"token\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede observarse que para el primer tweet (id = 1.195196e+17), todos los t√©rminos que aparecen una vez, tienen el mismo valor de tf, sin embargo, dado que no todos los t√©rminos aparecen con la misma frecuencia en el conjunto de todos los tweets, la correcci√≥n de idf es distinta para cada uno.\n",
    "\n",
    "De nuevo remarcar que, si bien se ha realizado el c√°lculo de forma manual con fines ilustrativos, en la pr√°ctica, es preferible utilizar implementaciones optimizadas como es el caso de la clase TfidfVectorizer de Scikit Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificaci√≥n de mensajes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder aplicar algoritmos de clasificaci√≥n a un texto, es necesario crear una representaci√≥n num√©rica del mismo. Una de las formas m√°s utilizadas se conoce como Bag of Words. Este m√©todo consiste en identificar el set formado por todas las palabras (tokens) que aparecen en el corpus, en este caso el conjunto de todos los tweets recuperados. Con este set se crea un espacio n-dimensional en el que cada dimensi√≥n (columna) es una palabra. Por √∫ltimo, se proyecta cada texto en ese espacio, asignando un valor para cada dimensi√≥n. En la mayor√≠a de casos, el valor utilizado es el tf-idf.\n",
    "\n",
    "En el siguiente apartado se construye un modelo de aprendizaje estad√≠stico basado en m√°quinas de vector soporte (SVM) con el objetivo de predecir la autor√≠a de los tweets. En concreto, se comparan los tweets de Elon Musk y Mayor Ed Lee.\n",
    "\n",
    "Como modelo se emplea un SVM de Scikit-Learn. Para facilitar la obtenci√≥n de la matriz TF-IDF se recurre a la clase TfidVectorized tambi√©n de Scikit-Learn pero, en lugar de utilizar el tokenizador por defecto, se emplea el mismo definido en los apartados anteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparto train y test\n",
    "# ==============================================================================\n",
    "datos_X = df['mensaje']\n",
    "datos_y = df['sentimiento']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    datos_X,\n",
    "    datos_y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 123\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': 13.125, 'fear': 11.1875, 'joy': 35.0, 'love': 7.875, 'sadness': 29.25, 'surprise': 3.5625}\n",
      "{'anger': 16.25, 'fear': 11.25, 'joy': 33.75, 'love': 8.25, 'sadness': 28.25, 'surprise': 2.25}\n"
     ]
    }
   ],
   "source": [
    "value, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(value, 100 * counts / sum(counts))))\n",
    "value, counts = np.unique(y_test, return_counts=True)\n",
    "print(dict(zip(value, 100 * counts / sum(counts))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizaci√≥n tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empleando los tweets de entrenamiento se crea un matriz tf-idf en la que cada columna es un t√©rmino, cada fila un documento y el valor de intersecci√≥n el tf-idf correspondiente. Esta matriz representa el espacio n-dimensional en el que se proyecta cada tweet.\n",
    "\n",
    "La clase TfidfVectorizer de Scikit Learn automatizan la creaci√≥n de una matriz df-idf a partir de un corpus de documentos. Entre sus argumentos destaca:\n",
    "\n",
    "encoding: el tipo de codificaci√≥n del texto, por defecto es 'utf-8'.\n",
    "\n",
    "strip_accents: eliminaci√≥n de acentos sustituyendolos por la misma letra sin el acento. Por defecto se emplea el m√©todo ‚Äòascii‚Äô.\n",
    "\n",
    "lowercase: convertir a min√∫sculas todo el texto.\n",
    "\n",
    "tokenizer: en caso de querer pasar un tokenizador definido por el usuario o de otra librer√≠a.\n",
    "\n",
    "analyzer: tipo de divisi√≥n que realiza el tokenizador. Por defecto separa por palabras ('word').\n",
    "\n",
    "stop_words: lista de stopwords que se eliminan durante el tokenizado. Por defecto utiliza un listado para el ingl√©s.\n",
    "\n",
    "ngram_range: rango de n-gramas incluidos. Por ejemplo, (1, 2) significa que se incluyen unigramas (palabras individuales) y bigramas (pares de palabras) como tokens.\n",
    "\n",
    "min_df: fracci√≥n o n√∫mero de documentos en los que debe de aparecer como m√≠nimo un t√©rmino para no ser excluido en el tokenizado. Este filtrado es una forma de eliminar ruido del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(min_df=3,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                tokenizer=<function limpiar_tokenizar at 0x7fa5abed6af0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creaci√≥n de la matriz tf-idf\n",
    "# ==============================================================================\n",
    "tfidf_vectorizador = TfidfVectorizer(\n",
    "                        tokenizer  = limpiar_tokenizar,\n",
    "                        min_df     = 3,\n",
    "                        stop_words = stop_words\n",
    "                    )\n",
    "tfidf_vectorizador.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = tfidf_vectorizador.transform(X_train)\n",
    "tfidf_test  = tfidf_vectorizador.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " N√∫mero de tokens creados: 1055\n",
      "['able', 'absolutely', 'acceptable', 'accepted', 'accepting', 'aching', 'act', 'actually', 'add', 'admit']\n"
     ]
    }
   ],
   "source": [
    "print(f\" N√∫mero de tokens creados: {len(tfidf_vectorizador.get_feature_names())}\")\n",
    "print(tfidf_vectorizador.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo SVM lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como modelo de predicci√≥n se emplea un SVM. Para m√°s informaci√≥n sobre c√≥mo entrenar modelos de Scikit learn consultar Machine learning con Python y Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del modelo SVM\n",
    "# ==============================================================================\n",
    "modelo_svm_lineal = svm.SVC(kernel= \"linear\", C = 1.0)\n",
    "modelo_svm_lineal.fit(X=tfidf_train, y= y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_C</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.154435</td>\n",
       "      <td>0.696875</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>0.981875</td>\n",
       "      <td>0.001809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.681005</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>0.025047</td>\n",
       "      <td>0.998906</td>\n",
       "      <td>0.000625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>129.154967</td>\n",
       "      <td>0.672500</td>\n",
       "      <td>0.028532</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.672500</td>\n",
       "      <td>0.028532</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.278256</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.012406</td>\n",
       "      <td>0.564063</td>\n",
       "      <td>0.007345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035938</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      param_C  mean_test_score  std_test_score  mean_train_score  \\\n",
       "6    2.154435         0.696875        0.011859          0.981875   \n",
       "7   16.681005         0.676250        0.025047          0.998906   \n",
       "8  129.154967         0.672500        0.028532          0.999375   \n",
       "9      1000.0         0.672500        0.028532          0.999375   \n",
       "5    0.278256         0.460000        0.012406          0.564063   \n",
       "0     0.00001         0.350000        0.000000          0.350000   \n",
       "1    0.000077         0.350000        0.000000          0.350000   \n",
       "2    0.000599         0.350000        0.000000          0.350000   \n",
       "3    0.004642         0.350000        0.000000          0.350000   \n",
       "4    0.035938         0.350000        0.000000          0.350000   \n",
       "\n",
       "   std_train_score  \n",
       "6         0.001809  \n",
       "7         0.000625  \n",
       "8         0.000312  \n",
       "9         0.000312  \n",
       "5         0.007345  \n",
       "0         0.000000  \n",
       "1         0.000000  \n",
       "2         0.000000  \n",
       "3         0.000000  \n",
       "4         0.000000  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid de hiperpar√°metros\n",
    "# ==============================================================================\n",
    "param_grid = {'C': np.logspace(-5, 3, 10)}\n",
    "\n",
    "# B√∫squeda por validaci√≥n cruzada\n",
    "# ==============================================================================\n",
    "grid = GridSearchCV(\n",
    "        estimator  = svm.SVC(kernel= \"linear\"),\n",
    "        param_grid = param_grid,\n",
    "        scoring    = 'accuracy',\n",
    "        n_jobs     = -1,\n",
    "        cv         = 5, \n",
    "        verbose    = 0,\n",
    "        return_train_score = True\n",
    "      )\n",
    "\n",
    "# Se asigna el resultado a _ para que no se imprima por pantalla\n",
    "_ = grid.fit(X = tfidf_train, y = y_train)\n",
    "\n",
    "# Resultados del grid\n",
    "# ==============================================================================\n",
    "resultados = pd.DataFrame(grid.cv_results_)\n",
    "resultados.filter(regex = '(param.*|mean_t|std_t)')\\\n",
    "    .drop(columns = 'params')\\\n",
    "    .sort_values('mean_test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Mejores hiperpar√°metros encontrados (cv)\n",
      "----------------------------------------\n",
      "{'C': 2.154434690031882} : 0.696875 accuracy\n"
     ]
    }
   ],
   "source": [
    "# Mejores hiperpar√°metros por validaci√≥n cruzada\n",
    "# ==============================================================================\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Mejores hiperpar√°metros encontrados (cv)\")\n",
    "print(\"----------------------------------------\")\n",
    "print(grid.best_params_, \":\", grid.best_score_, grid.scoring)\n",
    "\n",
    "modelo_final = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Error de test\n",
      "-------------\n",
      "N√∫mero de clasificaciones err√≥neas de un total de 400 clasificaciones: 108\n",
      "% de error: 27.0\n",
      "\n",
      "-------------------\n",
      "Matriz de confusi√≥n\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# Error predicciones test\n",
    "# ==============================================================================\n",
    "predicciones_test = modelo_final.predict(X=tfidf_test)\n",
    "\n",
    "print(\"-------------\")\n",
    "print(\"Error de test\")\n",
    "print(\"-------------\")\n",
    "\n",
    "print(f\"N√∫mero de clasificaciones err√≥neas de un total de {tfidf_test.shape[0]} \" \\\n",
    "      f\"clasificaciones: {(y_test != predicciones_test).sum()}\"\n",
    ")\n",
    "print(f\"% de error: {100*(y_test != predicciones_test).mean()}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"-------------------\")\n",
    "print(\"Matriz de confusi√≥n\")\n",
    "print(\"-------------------\")\n",
    "# pd.DataFrame(confusion_matrix(y_true = y_test, y_pred= predicciones_test),\n",
    "            # columns= [\"mensaje\", \"sentimiento\"],\n",
    "             #index = [\"mensaje\", \"sentimiento\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An√°lisis de sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma de analizar el sentimiento de un de un texto es considerando su sentimiento como la suma de los sentimientos de cada una de las palabras que lo forman. Esta no es la √∫nica forma de abordar el an√°lisis de sentimientos, pero consigue un buen equilibrio entre complejidad y resultados.\n",
    "\n",
    "Para llevar a cabo esta aproximaci√≥n es necesario disponer de un diccionario en el que se asocie a cada palabra un sentimiento o nivel de sentimiento. A estos diccionarios tambi√©n se les conoce como sentiment lexicon. Dos de los m√°s utilizados son:\n",
    "\n",
    "AFINN: en √©l, se asigna a cada palabra un valor entre -5 y 5, siendo -5 el m√°ximo de negatividad y +5 el m√°ximo de positividad. Se puede acceder al diccionario a trav√©s del repositorio https://github.com/fnielsen/afinn/tree/master/afinn/data.\n",
    "\n",
    "Bing y Liu (http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar), donde las palabras est√°n clasificadas como positivas o negativas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>termino</th>\n",
       "      <th>sentimiento2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandons</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abducted</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abduction</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     termino  sentimiento2\n",
       "0    abandon            -2\n",
       "1  abandoned            -2\n",
       "2   abandons            -2\n",
       "3   abducted            -2\n",
       "4  abduction            -2"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descarga lexicon sentimientos\n",
    "# ==============================================================================\n",
    "lexicon = pd.read_table(\n",
    "            'https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt',\n",
    "            names = ['termino', 'sentimiento2']\n",
    "          )\n",
    "lexicon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentimiento promedio de cada mensaje\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>sentimiento2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abilities</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ability</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abuse</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abused</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  sentimiento2\n",
       "0  abandoned            -6\n",
       "1  abilities             4\n",
       "2    ability             4\n",
       "3      abuse            -9\n",
       "4     abused            -6"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentimiento promedio de cada tweet\n",
    "# ==============================================================================\n",
    "df_sentimientos = pd.merge(\n",
    "                            left     = df_tidy,\n",
    "                            right    = lexicon,\n",
    "                            left_on  = \"token\", \n",
    "                            right_on = \"termino\",\n",
    "                            how      = \"inner\"\n",
    "                      )\n",
    "\n",
    "df_sentimientos = df_sentimientos.drop(columns = \"termino\")\n",
    "\n",
    "# Se suman los sentimientos de las palabras que forman cada tweet.\n",
    "df_sentimientos = df_sentimientos[[\"token\", \"sentimiento2\"]] \\\n",
    "                      .groupby([\"token\"])\\\n",
    "                      .sum().reset_index()\n",
    "df_sentimientos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mensajes positivos, negativos y neutros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============\n",
      "Positivos: 44.33\n",
      "Neutros  : 0.0\n",
      "Negativos: 55.67\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def perfil_sentimientos(df):\n",
    "    print(\"=\" * 12)\n",
    "    print(f\"Positivos: {round(100 * np.mean(df.sentimiento2 > 0), 2)}\")\n",
    "    print(f\"Neutros  : {round(100 * np.mean(df.sentimiento2 == 0), 2)}\")\n",
    "    print(f\"Negativos: {round(100 * np.mean(df.sentimiento2 < 0), 2)}\")\n",
    "    print(\" \")\n",
    "\n",
    "perfil_sentimientos(df_sentimientos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
