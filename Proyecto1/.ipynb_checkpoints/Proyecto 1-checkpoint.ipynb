{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/juandiegocardona/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "#style.use('ggplot') or plt.style.use('ggplot')\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lectura de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mensaje</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling rather rotten so im not very ambiti...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im updating my blog because i feel shitty</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i never make her separate from me because i do...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was feeling a little vain when i did this one</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>i just keep feeling like someone is being unki...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>im feeling a little cranky negative after this...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>i feel that i am useful to my people and that ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>im feeling more comfortable with derby i feel ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>i feel all weird when i have to meet w people ...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                mensaje sentimiento\n",
       "0     im feeling rather rotten so im not very ambiti...     sadness\n",
       "1             im updating my blog because i feel shitty     sadness\n",
       "2     i never make her separate from me because i do...     sadness\n",
       "3     i left with my bouquet of red and yellow tulip...         joy\n",
       "4       i was feeling a little vain when i did this one     sadness\n",
       "...                                                 ...         ...\n",
       "1995  i just keep feeling like someone is being unki...       anger\n",
       "1996  im feeling a little cranky negative after this...       anger\n",
       "1997  i feel that i am useful to my people and that ...         joy\n",
       "1998  im feeling more comfortable with derby i feel ...         joy\n",
       "1999  i feel all weird when i have to meet w people ...        fear\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datos.txt',sep=';',header=None)\n",
    "df.columns = ['mensaje','sentimiento']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza y Tokenizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso de limpieza de texto, dentro del ámbito de text mining, consiste en eliminar del texto todo aquello que no aporte información sobre su temática, estructura o contenido. No existe una única forma de hacerlo, depende en gran medida de la finalidad del análisis y de la fuente de la que proceda el texto. Por ejemplo, en las redes sociales, los usuarios pueden escribir de la forma que quieran, lo que suele resultar en un uso elevado de abreviaturas y signos de puntuación. En este ejercicio, se procede a eliminar: patrones no informativos (urls de páginas web), signos de puntuación, etiquetas HTML, caracteres sueltos y números.\n",
    "\n",
    "Tokenizar un texto consiste en dividir el texto en las unidades que lo conforman, entendiendo por unidad el elemento más sencillo con significado propio para el análisis en cuestión, en este caso, las palabras.\n",
    "\n",
    "Existen múltiples librerías que automatizan en gran medida la limpieza y tokenización de texto, por ejemplo, la clase feature_extraction.text.CountVectorizer de Scikit Learn, nltk.tokenize o spaCy. A pesar de ello, para este ejemplo, se define una función que, si bien está menos optimizada, tiene la ventaja de poder adaptarse fácilmente dependiendo del tipo de texto analizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esto es 1 ejemplo de l'limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\n",
      "['esto', 'es', 'ejemplo', 'de', 'limpieza', 'de', 'texto', 'cienciadedatos', 'textmining']\n"
     ]
    }
   ],
   "source": [
    "def limpiar_tokenizar(mensaje):\n",
    "    '''\n",
    "    Esta función limpia y tokeniza el texto en palabras individuales.\n",
    "    El orden en el que se va limpiando el texto no es arbitrario.\n",
    "    El listado de signos de puntuación se ha obtenido de: print(string.punctuation)\n",
    "    y re.escape(string.punctuation)\n",
    "    '''\n",
    "    \n",
    "    # Se convierte todo el texto a minúsculas\n",
    "    nuevo_texto = mensaje.lower()\n",
    "    # Eliminación de páginas web (palabras que empiezan por \"http\")\n",
    "    nuevo_texto = re.sub('http\\S+', ' ', nuevo_texto)\n",
    "    # Eliminación de signos de puntuación\n",
    "    regex = '[\\\\!\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
    "    nuevo_texto = re.sub(regex , ' ', nuevo_texto)\n",
    "    # Eliminación de números\n",
    "    nuevo_texto = re.sub(\"\\d+\", ' ', nuevo_texto)\n",
    "    # Eliminación de espacios en blanco múltiples\n",
    "    nuevo_texto = re.sub(\"\\\\s+\", ' ', nuevo_texto)\n",
    "    # Tokenización por palabras individuales\n",
    "    nuevo_texto = nuevo_texto.split(sep = ' ')\n",
    "    # Eliminación de tokens con una longitud < 2\n",
    "    nuevo_texto = [token for token in nuevo_texto if len(token) > 1]\n",
    "    \n",
    "    return(nuevo_texto)\n",
    "\n",
    "test = \"Esto es 1 ejemplo de l'limpieza de6 TEXTO  https://t.co/rnHPgyhx4Z @cienciadedatos #textmining\"\n",
    "print(test)\n",
    "print(limpiar_tokenizar(mensaje=test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mensaje</th>\n",
       "      <th>mensaje_tokenizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im feeling rather rotten so im not very ambiti...</td>\n",
       "      <td>[im, feeling, rather, rotten, so, im, not, ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im updating my blog because i feel shitty</td>\n",
       "      <td>[im, updating, my, blog, because, feel, shitty]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i never make her separate from me because i do...</td>\n",
       "      <td>[never, make, her, separate, from, me, because...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i left with my bouquet of red and yellow tulip...</td>\n",
       "      <td>[left, with, my, bouquet, of, red, and, yellow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was feeling a little vain when i did this one</td>\n",
       "      <td>[was, feeling, little, vain, when, did, this, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             mensaje  \\\n",
       "0  im feeling rather rotten so im not very ambiti...   \n",
       "1          im updating my blog because i feel shitty   \n",
       "2  i never make her separate from me because i do...   \n",
       "3  i left with my bouquet of red and yellow tulip...   \n",
       "4    i was feeling a little vain when i did this one   \n",
       "\n",
       "                                  mensaje_tokenizado  \n",
       "0  [im, feeling, rather, rotten, so, im, not, ver...  \n",
       "1    [im, updating, my, blog, because, feel, shitty]  \n",
       "2  [never, make, her, separate, from, me, because...  \n",
       "3  [left, with, my, bouquet, of, red, and, yellow...  \n",
       "4  [was, feeling, little, vain, when, did, this, ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se aplica la función de limpieza y tokenización a cada mensaje\n",
    "# ==============================================================================\n",
    "df['mensaje_tokenizado'] = df['mensaje'].apply(lambda x: limpiar_tokenizar(x))\n",
    "df[['mensaje', 'mensaje_tokenizado']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Análisis exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de entender que caracteriza la escritura de cada autor, es interesante estudiar qué palabras emplea, con qué frecuencia, así como el significado de las mismas.\n",
    "\n",
    "En Python, una de las estructuras que más facilita el análisis exploratorio es el DataFrame de Pandas, que es la estructura en la que se encuentra almacenada ahora la información de los tweets. Sin embargo, al realizar la tokenización, ha habido un cambio importante. Antes de dividir el texto, los elementos de estudio eran los tweets, y cada uno se encontraba en una fila, cumplimento así la condición de tidy data: una observación, una fila. Al realizar la tokenización, el elemento de estudio ha pasado a ser cada token (palabra), incumpliendo así la condición de tidy data. Para volver de nuevo a la estructura ideal se tiene que expandir cada lista de tokens, duplicando el valor de las otras columnas tantas veces como sea necesario. A este proceso se le conoce como expansión o unnest.\n",
    "\n",
    "Aunque puede parecer un proceso poco eficiente (el número de filas aumenta mucho), este simple cambio facilita actividades de tipo: agrupación, contaje, gráficos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentimiento</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>im</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>feeling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "      <td>rather</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentimiento    token\n",
       "0     sadness       im\n",
       "0     sadness  feeling\n",
       "0     sadness   rather"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unnest de la columna texto_tokenizado\n",
    "# ==============================================================================\n",
    "df_tidy = df.explode(column='mensaje_tokenizado')\n",
    "df_tidy = df_tidy.drop(columns='mensaje')\n",
    "df_tidy = df_tidy.rename(columns={'mensaje_tokenizado':'token'})\n",
    "df_tidy.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras totales utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34073"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tidy['token'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras totales distintas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4778"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tidy['token'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palabras más utilizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    " # df_tidy.groupby(['token'])[\"token\"] \\\n",
    " # .count() \\\n",
    " # .reset_index(name='count') \\\n",
    " # .apply(lambda x: x.sort_values('count', ascending=False).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la tabla anterior puede observarse que los términos más frecuentes en todos los usuarios se corresponden con artículos, preposiciones, pronombres…, en general, palabras que no aportan información relevante sobre el texto. Ha estas palabras se les conoce como stopwords. Para cada idioma existen distintos listados de stopwords, además, dependiendo del contexto, puede ser necesario adaptar el listado. Por ejemplo, en la tabla anterior aparece el término amp que procede de la etiqueta html &amp. Con frecuencia, a medida que se realiza un análisis se encuentran palabras que deben incluirse en el listado de stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "# Obtención de listado de stopwords del inglés\n",
    "# ==============================================================================\n",
    "stop_words = list(stopwords.words('english'))\n",
    "# Se añade la stoprword: amp, ax, ex\n",
    "stop_words.extend((\"amp\", \"xa\", \"xe\"))\n",
    "print(stop_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrado para excluir stopwords\n",
    "# ==============================================================================\n",
    "df_tidy = df_tidy[~(df_tidy[\"token\"].isin(stop_words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Top 10 palabras (sin stopwords)\n",
    "# ==============================================================================\n",
    "fig, axs = plt.subplots(nrows=1, ncols=1,figsize=(6, 7))\n",
    "counts  = df_tidy['token'].value_counts(ascending=False).head(10)\n",
    "counts.plot(kind='barh', color='firebrick')\n",
    "axs.invert_yaxis()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency e Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de los principales intereses en text mining, natural language processing e information retrieval es cuantificar la temática de un texto, así como la importancia de cada término que lo forma. Una manera sencilla de medir la importancia de un término dentro de un documento es utilizando la frecuencia con la que aparece (tf, term-frequency). Esta aproximación, aunque simple, tiene la limitación de atribuir mucha importancia a aquellas palabras que aparecen muchas veces aunque no aporten información selectiva. Por ejemplo, si la palabra matemáticas aparece 5 veces en un documento y la palabra página aparece 50, la segunda tendrá 10 veces más peso a pesar de que no aporte tanta información sobre la temática del documento. Para solucionar este problema se pueden ponderar los valores tf multiplicándolos por la inversa de la frecuencia con la que el término en cuestión aparece en el resto de documentos(idf). De esta forma, se consigue reducir el valor de aquellos términos que aparecen en muchos documentos y que, por lo tanto, no aportan información selectiva.\n",
    "\n",
    "El estadístico tf-idf mide cómo de informativo es un término en un documento teniendo en cuenta la frecuencia con la que ese término aparece en otros documentos.\n",
    "\n",
    "\n",
    "\n",
    "Term Frequency (tf)\n",
    "\n",
    "tf (t, d)=𝑛tlongitud d\n",
    " \n",
    "donde  𝑛t  es el número de veces que aparece el término  𝑡  en el documento  𝑑 .\n",
    "\n",
    "Inverse Document Frequency\n",
    "\n",
    "idf (t)=log(𝑛d𝑛(d,t))\n",
    " \n",
    "donde  𝑛d  es el número total de documentos y  𝑛(d,t)  el número de documentos que contienen el término  𝑡 .\n",
    "\n",
    "Estadístico tf-idf\n",
    "\n",
    "tf-idf(t, d)=tf (t, d)∗idf (t)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "En la práctica, para evitar problemas con el logaritmo cuando aparecen valores de 0, se emplea una versión corregida del  idf (t) . Esta es la versión implementada en Scikit Learn.\n",
    "\n",
    "idf (t)=log1+𝑛𝑑1+𝑛(d,t)+1\n",
    " \n",
    "\n",
    "\n",
    "En los siguientes apartados se muestra cómo calcular el valor tf-idf Sin embargo, en la práctica, es preferible utilizar implementaciones como TfidfVectorizer de Scikit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>tf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaah</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3099</th>\n",
       "      <td>powers</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>powerless</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token  count  total_count   tf\n",
       "0         aaaah      1            1  1.0\n",
       "3099     powers      1            1  1.0\n",
       "3098  powerless      1            1  1.0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cálculo term-frecuency (tf)\n",
    "# ==============================================================================\n",
    "tf = df_tidy.copy()\n",
    "# Número de veces que aparece cada término en cada tweet\n",
    "tf = tf.groupby([\"token\"])[\"token\"].agg([\"count\"]).reset_index()\n",
    "# Se añade una columna con el total de términos por tweet\n",
    "tf['total_count'] = tf.groupby('token')['count'].transform(sum)\n",
    "# Se calcula el tf\n",
    "tf['tf'] = tf[\"count\"] / tf[\"total_count\"]\n",
    "tf.sort_values(by = \"tf\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>n_documentos</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>feel</td>\n",
       "      <td>1394</td>\n",
       "      <td>1.203399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>feeling</td>\n",
       "      <td>646</td>\n",
       "      <td>1.972532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>like</td>\n",
       "      <td>373</td>\n",
       "      <td>2.521753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        token  n_documentos       idf\n",
       "1515     feel          1394  1.203399\n",
       "1516  feeling           646  1.972532\n",
       "2371     like           373  2.521753"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inverse document frequency\n",
    "# ==============================================================================\n",
    "idf = df_tidy.copy()\n",
    "total_documents = idf[\"token\"].drop_duplicates().count()\n",
    "# Número de documentos (tweets) en los que aparece cada término\n",
    "idf = idf.groupby([\"token\"])[\"token\"].agg([\"count\"]).reset_index()\n",
    "idf['n_documentos'] = idf.groupby('token')['count'].transform(sum)\n",
    "# Cálculo del idf\n",
    "idf['idf'] = np.log(total_documents / idf['n_documentos'])\n",
    "idf = idf[[\"token\",\"n_documentos\", \"idf\"]].drop_duplicates()\n",
    "idf.sort_values(by=\"idf\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "      <th>total_count</th>\n",
       "      <th>tf</th>\n",
       "      <th>n_documentos</th>\n",
       "      <th>idf</th>\n",
       "      <th>tf_idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaaah</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.443331</td>\n",
       "      <td>8.443331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7.344719</td>\n",
       "      <td>7.344719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandoning</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.443331</td>\n",
       "      <td>8.443331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.443331</td>\n",
       "      <td>8.443331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abba</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.443331</td>\n",
       "      <td>8.443331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token  count  total_count   tf  n_documentos       idf    tf_idf\n",
       "0        aaaah      1            1  1.0             1  8.443331  8.443331\n",
       "1    abandoned      3            3  1.0             3  7.344719  7.344719\n",
       "2   abandoning      1            1  1.0             1  8.443331  8.443331\n",
       "3  abandonment      1            1  1.0             1  8.443331  8.443331\n",
       "4         abba      1            1  1.0             1  8.443331  8.443331"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency - Inverse Document Frequency\n",
    "# ==============================================================================\n",
    "tf_idf = pd.merge(left=tf, right=idf, on=\"token\")\n",
    "tf_idf[\"tf_idf\"] = tf_idf[\"tf\"] * tf_idf[\"idf\"]\n",
    "tf_idf.sort_values(by=\"token\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede observarse que para el primer tweet (id = 1.195196e+17), todos los términos que aparecen una vez, tienen el mismo valor de tf, sin embargo, dado que no todos los términos aparecen con la misma frecuencia en el conjunto de todos los tweets, la corrección de idf es distinta para cada uno.\n",
    "\n",
    "De nuevo remarcar que, si bien se ha realizado el cálculo de forma manual con fines ilustrativos, en la práctica, es preferible utilizar implementaciones optimizadas como es el caso de la clase TfidfVectorizer de Scikit Learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de mensajes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder aplicar algoritmos de clasificación a un texto, es necesario crear una representación numérica del mismo. Una de las formas más utilizadas se conoce como Bag of Words. Este método consiste en identificar el set formado por todas las palabras (tokens) que aparecen en el corpus, en este caso el conjunto de todos los tweets recuperados. Con este set se crea un espacio n-dimensional en el que cada dimensión (columna) es una palabra. Por último, se proyecta cada texto en ese espacio, asignando un valor para cada dimensión. En la mayoría de casos, el valor utilizado es el tf-idf.\n",
    "\n",
    "En el siguiente apartado se construye un modelo de aprendizaje estadístico basado en máquinas de vector soporte (SVM) con el objetivo de predecir la autoría de los tweets. En concreto, se comparan los tweets de Elon Musk y Mayor Ed Lee.\n",
    "\n",
    "Como modelo se emplea un SVM de Scikit-Learn. Para facilitar la obtención de la matriz TF-IDF se recurre a la clase TfidVectorized también de Scikit-Learn pero, en lugar de utilizar el tokenizador por defecto, se emplea el mismo definido en los apartados anteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reparto train y test\n",
    "# ==============================================================================\n",
    "datos_X = df['mensaje']\n",
    "datos_y = df['sentimiento']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    datos_X,\n",
    "    datos_y,\n",
    "    test_size = 0.2,\n",
    "    random_state = 123\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'anger': 13.125, 'fear': 11.1875, 'joy': 35.0, 'love': 7.875, 'sadness': 29.25, 'surprise': 3.5625}\n",
      "{'anger': 16.25, 'fear': 11.25, 'joy': 33.75, 'love': 8.25, 'sadness': 28.25, 'surprise': 2.25}\n"
     ]
    }
   ],
   "source": [
    "value, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(value, 100 * counts / sum(counts))))\n",
    "value, counts = np.unique(y_test, return_counts=True)\n",
    "print(dict(zip(value, 100 * counts / sum(counts))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empleando los tweets de entrenamiento se crea un matriz tf-idf en la que cada columna es un término, cada fila un documento y el valor de intersección el tf-idf correspondiente. Esta matriz representa el espacio n-dimensional en el que se proyecta cada tweet.\n",
    "\n",
    "La clase TfidfVectorizer de Scikit Learn automatizan la creación de una matriz df-idf a partir de un corpus de documentos. Entre sus argumentos destaca:\n",
    "\n",
    "encoding: el tipo de codificación del texto, por defecto es 'utf-8'.\n",
    "\n",
    "strip_accents: eliminación de acentos sustituyendolos por la misma letra sin el acento. Por defecto se emplea el método ‘ascii’.\n",
    "\n",
    "lowercase: convertir a minúsculas todo el texto.\n",
    "\n",
    "tokenizer: en caso de querer pasar un tokenizador definido por el usuario o de otra librería.\n",
    "\n",
    "analyzer: tipo de división que realiza el tokenizador. Por defecto separa por palabras ('word').\n",
    "\n",
    "stop_words: lista de stopwords que se eliminan durante el tokenizado. Por defecto utiliza un listado para el inglés.\n",
    "\n",
    "ngram_range: rango de n-gramas incluidos. Por ejemplo, (1, 2) significa que se incluyen unigramas (palabras individuales) y bigramas (pares de palabras) como tokens.\n",
    "\n",
    "min_df: fracción o número de documentos en los que debe de aparecer como mínimo un término para no ser excluido en el tokenizado. Este filtrado es una forma de eliminar ruido del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(min_df=3,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                tokenizer=<function limpiar_tokenizar at 0x7fa5abed6af0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creación de la matriz tf-idf\n",
    "# ==============================================================================\n",
    "tfidf_vectorizador = TfidfVectorizer(\n",
    "                        tokenizer  = limpiar_tokenizar,\n",
    "                        min_df     = 3,\n",
    "                        stop_words = stop_words\n",
    "                    )\n",
    "tfidf_vectorizador.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train = tfidf_vectorizador.transform(X_train)\n",
    "tfidf_test  = tfidf_vectorizador.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Número de tokens creados: 1055\n",
      "['able', 'absolutely', 'acceptable', 'accepted', 'accepting', 'aching', 'act', 'actually', 'add', 'admit']\n"
     ]
    }
   ],
   "source": [
    "print(f\" Número de tokens creados: {len(tfidf_vectorizador.get_feature_names())}\")\n",
    "print(tfidf_vectorizador.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo SVM lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como modelo de predicción se emplea un SVM. Para más información sobre cómo entrenar modelos de Scikit learn consultar Machine learning con Python y Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenamiento del modelo SVM\n",
    "# ==============================================================================\n",
    "modelo_svm_lineal = svm.SVC(kernel= \"linear\", C = 1.0)\n",
    "modelo_svm_lineal.fit(X=tfidf_train, y= y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_C</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.154435</td>\n",
       "      <td>0.696875</td>\n",
       "      <td>0.011859</td>\n",
       "      <td>0.981875</td>\n",
       "      <td>0.001809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16.681005</td>\n",
       "      <td>0.676250</td>\n",
       "      <td>0.025047</td>\n",
       "      <td>0.998906</td>\n",
       "      <td>0.000625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>129.154967</td>\n",
       "      <td>0.672500</td>\n",
       "      <td>0.028532</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.672500</td>\n",
       "      <td>0.028532</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.278256</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.012406</td>\n",
       "      <td>0.564063</td>\n",
       "      <td>0.007345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035938</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      param_C  mean_test_score  std_test_score  mean_train_score  \\\n",
       "6    2.154435         0.696875        0.011859          0.981875   \n",
       "7   16.681005         0.676250        0.025047          0.998906   \n",
       "8  129.154967         0.672500        0.028532          0.999375   \n",
       "9      1000.0         0.672500        0.028532          0.999375   \n",
       "5    0.278256         0.460000        0.012406          0.564063   \n",
       "0     0.00001         0.350000        0.000000          0.350000   \n",
       "1    0.000077         0.350000        0.000000          0.350000   \n",
       "2    0.000599         0.350000        0.000000          0.350000   \n",
       "3    0.004642         0.350000        0.000000          0.350000   \n",
       "4    0.035938         0.350000        0.000000          0.350000   \n",
       "\n",
       "   std_train_score  \n",
       "6         0.001809  \n",
       "7         0.000625  \n",
       "8         0.000312  \n",
       "9         0.000312  \n",
       "5         0.007345  \n",
       "0         0.000000  \n",
       "1         0.000000  \n",
       "2         0.000000  \n",
       "3         0.000000  \n",
       "4         0.000000  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid de hiperparámetros\n",
    "# ==============================================================================\n",
    "param_grid = {'C': np.logspace(-5, 3, 10)}\n",
    "\n",
    "# Búsqueda por validación cruzada\n",
    "# ==============================================================================\n",
    "grid = GridSearchCV(\n",
    "        estimator  = svm.SVC(kernel= \"linear\"),\n",
    "        param_grid = param_grid,\n",
    "        scoring    = 'accuracy',\n",
    "        n_jobs     = -1,\n",
    "        cv         = 5, \n",
    "        verbose    = 0,\n",
    "        return_train_score = True\n",
    "      )\n",
    "\n",
    "# Se asigna el resultado a _ para que no se imprima por pantalla\n",
    "_ = grid.fit(X = tfidf_train, y = y_train)\n",
    "\n",
    "# Resultados del grid\n",
    "# ==============================================================================\n",
    "resultados = pd.DataFrame(grid.cv_results_)\n",
    "resultados.filter(regex = '(param.*|mean_t|std_t)')\\\n",
    "    .drop(columns = 'params')\\\n",
    "    .sort_values('mean_test_score', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Mejores hiperparámetros encontrados (cv)\n",
      "----------------------------------------\n",
      "{'C': 2.154434690031882} : 0.696875 accuracy\n"
     ]
    }
   ],
   "source": [
    "# Mejores hiperparámetros por validación cruzada\n",
    "# ==============================================================================\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Mejores hiperparámetros encontrados (cv)\")\n",
    "print(\"----------------------------------------\")\n",
    "print(grid.best_params_, \":\", grid.best_score_, grid.scoring)\n",
    "\n",
    "modelo_final = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "Error de test\n",
      "-------------\n",
      "Número de clasificaciones erróneas de un total de 400 clasificaciones: 108\n",
      "% de error: 27.0\n",
      "\n",
      "-------------------\n",
      "Matriz de confusión\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "# Error predicciones test\n",
    "# ==============================================================================\n",
    "predicciones_test = modelo_final.predict(X=tfidf_test)\n",
    "\n",
    "print(\"-------------\")\n",
    "print(\"Error de test\")\n",
    "print(\"-------------\")\n",
    "\n",
    "print(f\"Número de clasificaciones erróneas de un total de {tfidf_test.shape[0]} \" \\\n",
    "      f\"clasificaciones: {(y_test != predicciones_test).sum()}\"\n",
    ")\n",
    "print(f\"% de error: {100*(y_test != predicciones_test).mean()}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"-------------------\")\n",
    "print(\"Matriz de confusión\")\n",
    "print(\"-------------------\")\n",
    "# pd.DataFrame(confusion_matrix(y_true = y_test, y_pred= predicciones_test),\n",
    "            # columns= [\"mensaje\", \"sentimiento\"],\n",
    "             #index = [\"mensaje\", \"sentimiento\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una forma de analizar el sentimiento de un de un texto es considerando su sentimiento como la suma de los sentimientos de cada una de las palabras que lo forman. Esta no es la única forma de abordar el análisis de sentimientos, pero consigue un buen equilibrio entre complejidad y resultados.\n",
    "\n",
    "Para llevar a cabo esta aproximación es necesario disponer de un diccionario en el que se asocie a cada palabra un sentimiento o nivel de sentimiento. A estos diccionarios también se les conoce como sentiment lexicon. Dos de los más utilizados son:\n",
    "\n",
    "AFINN: en él, se asigna a cada palabra un valor entre -5 y 5, siendo -5 el máximo de negatividad y +5 el máximo de positividad. Se puede acceder al diccionario a través del repositorio https://github.com/fnielsen/afinn/tree/master/afinn/data.\n",
    "\n",
    "Bing y Liu (http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar), donde las palabras están clasificadas como positivas o negativas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>termino</th>\n",
       "      <th>sentimiento2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abandons</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abducted</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abduction</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     termino  sentimiento2\n",
       "0    abandon            -2\n",
       "1  abandoned            -2\n",
       "2   abandons            -2\n",
       "3   abducted            -2\n",
       "4  abduction            -2"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descarga lexicon sentimientos\n",
    "# ==============================================================================\n",
    "lexicon = pd.read_table(\n",
    "            'https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt',\n",
    "            names = ['termino', 'sentimiento2']\n",
    "          )\n",
    "lexicon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentimiento promedio de cada mensaje\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>sentimiento2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abilities</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ability</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abuse</td>\n",
       "      <td>-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abused</td>\n",
       "      <td>-6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       token  sentimiento2\n",
       "0  abandoned            -6\n",
       "1  abilities             4\n",
       "2    ability             4\n",
       "3      abuse            -9\n",
       "4     abused            -6"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentimiento promedio de cada tweet\n",
    "# ==============================================================================\n",
    "df_sentimientos = pd.merge(\n",
    "                            left     = df_tidy,\n",
    "                            right    = lexicon,\n",
    "                            left_on  = \"token\", \n",
    "                            right_on = \"termino\",\n",
    "                            how      = \"inner\"\n",
    "                      )\n",
    "\n",
    "df_sentimientos = df_sentimientos.drop(columns = \"termino\")\n",
    "\n",
    "# Se suman los sentimientos de las palabras que forman cada tweet.\n",
    "df_sentimientos = df_sentimientos[[\"token\", \"sentimiento2\"]] \\\n",
    "                      .groupby([\"token\"])\\\n",
    "                      .sum().reset_index()\n",
    "df_sentimientos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mensajes positivos, negativos y neutros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============\n",
      "Positivos: 44.33\n",
      "Neutros  : 0.0\n",
      "Negativos: 55.67\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def perfil_sentimientos(df):\n",
    "    print(\"=\" * 12)\n",
    "    print(f\"Positivos: {round(100 * np.mean(df.sentimiento2 > 0), 2)}\")\n",
    "    print(f\"Neutros  : {round(100 * np.mean(df.sentimiento2 == 0), 2)}\")\n",
    "    print(f\"Negativos: {round(100 * np.mean(df.sentimiento2 < 0), 2)}\")\n",
    "    print(\" \")\n",
    "\n",
    "perfil_sentimientos(df_sentimientos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
